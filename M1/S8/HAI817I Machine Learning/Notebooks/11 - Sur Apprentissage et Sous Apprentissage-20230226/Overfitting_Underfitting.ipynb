{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Overfitting_Underfitting.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clMKjwK40IsV"
      },
      "source": [
        "<H1> Biais et Variance </H1>\n",
        "\n",
        "Le but de ce notebook est de mieux estimer la qualité du modèle. En apprentissage automatique, certains modèles sont vraiment trop simples et ne tiennent pas compte des relations qui peuvent exister entre les données d'apprentissage afin de pouvoir améliorer les prédictions. Ces modèles possèdent un **Biais élevé**. Au contraire, d'autres modèles sont trop complexes et en cherchant des relations dans les variables vont au final ajouter du bruit. Ainsi en modifiant un peu les données, les prédictions s'avèrent très différentes car le modèle est trop sensible et réagit de manière excessive au changement de données, i.e. **forte Variance**. Ils ne sont donc pas généralisable.\n",
        "La difficulté est donc d'arriver à trouver un bon compromis entre Biais et Variance comme l'illustre la figure suivante :  \n",
        "\n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/biais_variance.jpg\" align=\"center\" width=\"350\" height=\"350\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQSC8frX5Ao"
      },
      "source": [
        "## Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zICeCUqYCfS"
      },
      "source": [
        "\n",
        "Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :  \n",
        "\n",
        "*! pip install nom_librairie*  \n",
        "\n",
        "**Attention :** il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n",
        "\n",
        "**Remarque :** même si toutes les librairies sont importées dès le début, les librairies utiles pour des fonctions présentées au cours de ce notebook sont ré-importées de manière à indiquer d'où elles viennent et ainsi faciliter la réutilisation de la fonction dans un autre projet.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0OpnDBHYHmv"
      },
      "source": [
        "# utiliser cette cellule pour installer les librairies manquantes\n",
        "# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n",
        "# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n",
        "# recommencer tant que toutes les librairies ne sont pas installées ...\n",
        "\n",
        "# sous Colab il faut déjà intégrer ces deux librairies\n",
        "\n",
        "#!pip install  \n",
        "\n",
        "# eventuellement ne pas oublier de relancer le kernel du notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "\n",
        "#Sickit learn met régulièrement à jour des versions et \n",
        "#indique des futurs warnings. \n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# librairies générales\n",
        "import pickle \n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "import copy\n",
        "\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "\n",
        "# TensorFlow et keras\n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn import datasets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted')\n",
        "\n",
        "rcParams['figure.figsize'] = 20, 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s2sF7a-am4I"
      },
      "source": [
        "Pour pouvoir sauvegarder sur votre répertoire Google Drive, il est nécessaire de fournir une autorisation. Pour cela il suffit d'éxecuter la ligne suivante et de saisir le code donné par Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1zULJ9daqP6"
      },
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qe4Xtohau6v"
      },
      "source": [
        "Corriger éventuellement la ligne ci-dessous pour mettre le chemin vers un répertoire spécifique dans votre répertoire Google Drive : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahDl5RV9tVfp"
      },
      "source": [
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAYb7LUyDp0d"
      },
      "source": [
        "# fonctions utilities (affichage, confusion, etc.)\n",
        "from MyNLPUtilities import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R4vUZW80i3s"
      },
      "source": [
        "Fonction d'affichage des courbes loss et accuracy :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsIGUs4hUBKl"
      },
      "source": [
        "def plot_learningcurve(train_scores, test_scores):\n",
        "  #\n",
        "  # training and test mean and std\n",
        "  #\n",
        "  train_mean = np.mean(train_scores, axis=1)\n",
        "  train_std = np.std(train_scores, axis=1)\n",
        "  test_mean = np.mean(test_scores, axis=1)\n",
        "  test_std = np.std(test_scores, axis=1)\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  # Plot the learning curve\n",
        "\n",
        "  plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training Accuracy')\n",
        "  plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
        "  plt.plot(train_sizes, test_mean, color='green', marker='+', markersize=5, linestyle='--', label='Validation Accuracy')\n",
        "  plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
        "  plt.title('Learning Curve')\n",
        "  plt.xlabel('Training Data Size')\n",
        "  plt.ylabel('Model accuracy')\n",
        "  plt.grid()\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.show()\n",
        "\n",
        "def plot_curves_confusion (history,confusion_matrix,class_names):\n",
        "  plt.figure(1,figsize=(16,6))\n",
        "  plt.gcf().subplots_adjust(left = 0.125, bottom = 0.2, right = 1,\n",
        "                          top = 0.9, wspace = 0.25, hspace = 0)\n",
        "\n",
        "  # division de la fenêtre graphique en 1 ligne, 3 colonnes,\n",
        "  # graphique en position 1 - loss fonction\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training loss', 'Validation loss'], loc='upper left')\n",
        "  # graphique en position 2 - accuracy\n",
        "  plt.subplot(1,3,2)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['Training accuracy', 'Validation accuracy'], loc='upper left')\n",
        "  \n",
        "  # matrice de correlation\n",
        "  plt.subplot(1,3,3)\n",
        "  sns.heatmap(conf,annot=True,fmt=\"d\",cmap='Blues',xticklabels=class_names, yticklabels=class_names)# label=class_names)\n",
        "  # labels, title and ticks\n",
        "  plt.xlabel('Predicted', fontsize=12)\n",
        "  #plt.set_label_position('top') \n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  #plt.tick_top()\n",
        "  plt.title(\"Correlation matrix\")\n",
        "  plt.ylabel('True', fontsize=12)\n",
        "  #plt.set_ticklabels(class_names, fontsize = 8)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ylBcFNHrdti"
      },
      "source": [
        "# Bias-Variance Tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku-C9NNRtlo6"
      },
      "source": [
        "\n",
        "Les modèles à **biais élevé** sont *sous-adaptés* (**Underfit**) aux données d'apprentissage et l'erreur de prédiction est élevée à la fois sur les données d'entraînement et les données de test. Les modèles à **forte variance** sont *surajustés* (**Overfit**) aux données d'apprentissage et leur erreur de prédiction est généralement faible sur les données d'entraînement mais élevée sur les données de test, i.e. ils manquent de généralisation.\n",
        "\n",
        "La figure suivante illustre différents cas en considérant des approches de type régression (voir M1), de type modèles de classification traditionnels (voir M1) ou bien de modèles de deep learning. Les courbes obtenues permettent de mettre en évidence où se situe un modèle :  \n",
        "\n",
        "\n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/underoverfit.png\" align=\"center\" width=\"360\" height=\"360\">\n",
        "</center>\n",
        "(source : https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png)\n",
        "\n",
        "\n",
        "Les figures ci-dessous illustrent différents exemples d'**underfit**, d'**overfit** et de **goodfit** que nous allons voir. Elles sont données pour vous permettre, par la suite, de rapidement vérifier où se situe votre modèle et donc éventuellement de l'améliorer.  \n",
        "\n",
        "La figure ci-dessous illustre des exemples d'**underfit** en utilisant la courbe de *loss*: \n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/UnderfitImages.png\" align=\"center\" width=\"4000\" height=\"300\">\n",
        "</center>\n",
        "\n",
        "La partie *(a)* illustre un cas où le modèle est trop simple. Il n'est pas capable de prendre en compte la complexité du jeu de données. La partie *(b)* illustre un cas où la *loss* diminue et continue à diminuer jusqu'en bas. Cela indique que le modèle est capable d'apprendre encore et qu'il a été arrêté prématurément. Enfin la partie *(c)* montre également un exemple où le processus s'est arrêté trop tôt.\n",
        "\n",
        "En résumé, une courbe d'apprentissage montre un **underfitting** si :\n",
        "* La courbe *loss* reste stable quelque soit l'entraînement.\n",
        "* La courbe *loss* continue de diminuer jusqu'à la fin de l'entraînement. \n",
        "\n",
        "La figure ci-dessous illustre des exemples d'**overfit** : \n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/OverfitImages.png\" align=\"center\" width=\"4000\" height=\"300\">\n",
        "</center>\n",
        "\n",
        "La partie *(a)* illustre un exemple d'overfitting où nous voyons bien sur la courbe de *loss* qu'à un moment donné la validation ne perd plus rien. Le modèle apprend \"trop bien\" les données d'entrainement et même les bruits associés. Il faut, dans ce cas là, stopper l'entraînement plus tôt (e.g. Early Stop). La partie *(b)* illustre le même cas mais sur la courbe de l'*accuracy*. Enfin la partie *(c)* montre un overfitting qui démarre très tôt et où la courbe a une forme de *U*. Généralement cela indique le fait que le modèle apprend trop vite et que le learning rate est trop élevé. \n",
        "\n",
        "En résumé, les courbes d'apprentissage montrent un **overfitting** si :\n",
        "\n",
        "* La courbe *loss* pour l'entraînement continue de diminuer.\n",
        "* La courbe *loss* pour la validation diminue jusqu'à un certain point et recommence à augmenter.\n",
        "\n",
        "Remarque : le point d'inflexion de *loss* pour la validation peut être le point auquel l'apprentissage pourrait être interrompu.  \n",
        "\n",
        "Un **goodfitting** peut se reprérer lorque la *loss* pour l'apprentissage et le test diminue jusqu'à un point de stabilité avec un écart minimal entre les deux valeurs.  \n",
        "\n",
        "La figure ci-dessous illustre des exemples de **goodfitting** : \n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/GoodfitImages.png\" align=\"center\" width=\"4000\" height=\"300\">\n",
        "</center>\n",
        "\n",
        "La partie *(a)* montre que la *loss* décroit vers un point de stabilité aussi bien pour le test que pour l'apprentissage. L'écart entre les deux s'appelle « **écart de généralisation** ». La partie *(b)* illustre un exemple où, au début, la *loss* est assez faible pour l'entraînement et que pour la validation elle diminue rapidement puis \n",
        "très progressivement lors de l'ajout d'exemples d'apprentissage. Par la suite elle s'applatit diminuant ainsi l'écart de généralisation. \n",
        "Enfin, la partie *(c)* montre également que la *loss* pour l'apprentissage et le test décroit vers un point stable avec un écart de généralisation très faible.  \n",
        "\n",
        "En résumé, les courbes d'apprentissage montrent un **goodfitting** si :\n",
        "* La courbe *loss* pour l'entraînement diminue jusqu'à un point de stabilité.\n",
        "* La courbe *loss* pour la validation diminue jusqu'à un point de stabilité et présente un petit écart avec la *loss* pour l'entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNqXdkwS5_8A"
      },
      "source": [
        "\n",
        "**Solutions :**\n",
        "Il existe bien entendu des moyens pour corriger le modèle :   \n",
        "\n",
        " \n",
        "* **Underfitting :**  \n",
        "1. Comme l'underfitting est lié au fait que le modèle est trop simple, il ne capture pas les relations dans les données. Dans cette situation, la meilleure stratégie consiste à augmenter la complexité du modèle. Il peut s'agir d'ajouter d'autres features, par exemple en classification traditionnelle, ou bien d'augmenter le nombre de paramètres du modèle d'apprentissage profond. L'augmentation de la complexité du modèle va forcément entraîner une amélioration des performances. Il faut toutefois veiller à ne pas avoir une erreur d'entraînement de zéro, i.e. dans ce cas on arrive en **overfitting**.\n",
        "2. Augmentation du nombre d'epochs. Si le modèle montre qu'il peut apprendre plus, il est possible d'augmenter le nombre d'epochs. Comme la courbe montre qu'il faut du temps pour atteindre un minimum, il faut peut être aussi regarder le *learning rate* pour accélérer le processus.\n",
        "3. Bien vérifier que les données soient bien mélangées à chaque epoch. Penser à la validation croisée.   \n",
        "\n",
        "* **Overfitting :** \n",
        "1. Ajouter plus de données d'entraînement. Le fait d'avoir un ensemble de données plus important et diversifié aide généralement à améliorer les performances du modèle, i.e. un modèle qui généralise mieux. \n",
        "2. Augmentation. S'il n'est pas possible de rajouter des données, il est peut être possible d'utiliser les données présentes et d'en faire des variations. Pour des données numériques on peut penser à générer des données en tenant compte de la moyenne et de la variance initiales. Il existe par exemple pour les images des librairies spécifiques (déformation, rotation, zoom, etc.). La difficulté est d'avoir des données augmentées qui restent proches du réel. \n",
        "3. Early Stopping. Keras propose par exemple de pouvoir arrêter l'apprentissage lorsque des métriques (loss, accuracy, etc.) n'évoluent pas pendant un certain temps.\n",
        "4. Ajout de dropout. Les couches de type dropout permettent de \"bloquer\" des neurones différents à chaque étape pendant l'apprentissage et ainsi  éviter que le modèle soit trop proche des données. Attention les dropout sont utilisés lors de l'apprentissage et non pas avec lors du test de validation. La conséquence est que les courbes sont un peu différentes et que l'accuracy pour l'apprentissage peut être un peu moins bon. \n",
        "5. Régularisation. La régularisation est un terme supplémentaire ajouté à la fonction de perte pour imposer une pénalité sur les poids importants des paramètres de réseau afin de réduire le surapprentissage. Le dropout ou le early stopping sont des sortes de méthodes de régularisation. Il en existe d'autres. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcSKyI5oHal1"
      },
      "source": [
        "### Jeux de données non représentatif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL_HWPSBIhTo"
      },
      "source": [
        "Dans cette section, nous abordons des courbes qui peuvent apparaître parce que les jeux de données (apprentissage et validation) ne sont pas représentatifs. Cela peut arriver par exemple lors d'un *test_train_split* avec de mauvais paramètres si les classes ne sont pas équilibrées ou bien s'il manque des données.  \n",
        "\n",
        "\n",
        "Lors de l'apprentissage, le modèle va avoir un jeu d'apprentissage et un jeu de validation. Nous considérons les conséquences que cela peut avoir sur ces deux jeux.  \n",
        "\n",
        "**Jeu d'apprentissage non représentatif**  \n",
        "\n",
        "Un jeu d'apprentissage non représentatif signifie que l'ensemble de données d'apprentissage ne fournit pas suffisamment d'informations pour apprendre le problème, par rapport à l'ensemble des données de validation utilisée pour l'évaluer. Cela peut se produire si l'ensemble de données d'apprentissage comporte trop peu d'exemples par rapport à l'ensemble de données de validation.  \n",
        "\n",
        "La figure ci-dessous illustre des exemples de **jeux d'apprentissage non représentatifs** : \n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/UnrepresentativeTrain.png\" align=\"center\" width=\"550\" height=\"240\">\n",
        "</center>\n",
        "\n",
        "La partie *(a)* illustre un cas où les *loss* pour l'entraînement et la validation montrent une amélioration mais qu'il reste un écart important entre les deux courbes. La partie *(b)* montre un autre exemple ou il reste toujours un fort écart entre les deux *loss*.  \n",
        "\n",
        "Il existe différentes solutions.  \n",
        "**Solutions :**\n",
        "1. Ajouter plus de données. Il n'y a peut être pas assez de données pour capturer ce qu'il y a dans les données d'apprentissage et de validation. Eventuellement faire de la data augmentation : ajouts de données similaires, déformation d'images, etc. \n",
        "1. Vérifier que l'échantillonage est bien fait. Si les classes sont déséquilibrées s'assurer que les échantillons sont représentatifs, par exemple utiliser (*stratify=y*) dans train_test_split ou faire un *StratifiedKfold* plutôt qu'un Kfold. \n",
        "1. Faire de la cross validation pour être certain que toutes les données puissent être dans le jeu d'apprentissage et de validation.  \n",
        "\n",
        "**Jeu de validation non représentatif**  \n",
        "\n",
        "Un jeu de validation non représentatif signifie qu'il ne fournit pas suffisamment d'informations pour évaluer la capacité du modèle à généraliser. Cela peut se produire si l'ensemble de données de validation contient trop peu d'exemples par rapport à l'ensemble de données d'apprentissage. \n",
        "\n",
        "\n",
        "La figure ci-dessous illustre des exemples de **jeux de validation non représentatifs** : \n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/UnrepresentativeValidation.png\" align=\"center\" width=\"4000\" height=\"300\">\n",
        "</center>\n",
        "\n",
        "Les parties *(a)* et *(b)* illustrent deux cas où même si la *loss* d'entraînement se comporte bien, la *loss* en validation montre de grandes variations et pas ou peu d'amélioration.  La partie *(c)* montre une *loss* validation inférieure à la *loss* d'apprentissage. Cela indique que l'ensemble de données de validation peut être plus facile à prédire pour le modèle que l'ensemble de données d'apprentissage.\n",
        "\n",
        "Les solutions sont assez similaires aux précédentes.\n",
        "**Solutions :**\n",
        "1. Ajouter plus de données dans le jeu de validation (cf. taille de l'échantillon)\n",
        "1. Effectuer une cross validation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jC9sF6DxsGI"
      },
      "source": [
        "\n",
        "Dans ce notebook nous présentons les librairies ou les moyens qui peuvent être utilisés pour vérifier comment se situe un modèle. Nous allons dans un premier temps voir les learning curves de scikit learn et enfin nous examinons comment afficher les courbes dans le cas d'un réseau profond.   \n",
        "\n",
        "Nous illustrons, par la suite avec différentes jeux de données classiques des cas d'overfitting ou d'underfitting à l'aide de différents modèles et via différentes méthodes. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OllEP0AnyEOb"
      },
      "source": [
        "# Utilisation des learning curve (Scikit Learn)\n",
        "\n",
        "La courbe d'apprentissage est utilisée pour évaluer les performances des modèles avec un nombre variable d'échantillons d'apprentissage. Ceci est réalisé en surveillant les scores d'apprentissage et de validation (précision du modèle) avec un nombre croissant d'échantillons d'apprentissage. Dans la figure suivante, l'entraînement correspond à la ligne pointillée orange, la validation à la ligne bleue et la précision en  pointillée noire).  \n",
        "\n",
        "\n",
        "<center>\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/LearningCurves.png\" align=\"center\" width=\"500\" height=\"220\">\n",
        "</center>  \n",
        "\n",
        "La méthode *learning_curve* utilise comme paramètres un estimateur (classifieur), les jeux de données et la validation croisée. Il est également nécessaire de spécifier les tailles du train_sizes qui seront testées.  \n",
        "\n",
        "Par la suite, pour illustrer différents comportements nous utiliserons le jeu de donnéees digits disponible dans Scickit Learn. Il contient des données correspondant à des images de chiffres écrits à la main : 10 classes où chaque classe fait référence à un chiffre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs_sVEy_042v"
      },
      "source": [
        "dataset = datasets.load_digits()\n",
        "  \n",
        "# X contient les variables prédictives et y la variable à prédire\n",
        "X, y = dataset.data, dataset.target\n",
        "\n",
        "# normalisation du jeu de données\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vniY4fdBCrl"
      },
      "source": [
        "**Cas d'une regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy_M7gBb1TlE"
      },
      "source": [
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator=LogisticRegression(solver='lbfgs', random_state=1, max_iter=1000)\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=estimator, X=X, y=y,\n",
        "                                                       cv=cv, train_sizes=np.linspace(0.1, 1.0, 20),\n",
        "                                                     n_jobs=1)\n",
        "\n",
        "plot_learningcurve(train_scores,test_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2oPcZD7Fmij"
      },
      "source": [
        "La courbe montre que quelque soit la taille du jeu de données d'apprentissage le score sur l'apprentissage ne change pas. De la même manière pour le jeu de validation un palier est obtenu. C'est un cas typique d'**underfitting**. Le résulat était aussi un peu attendu. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5JMDC5p3Jb0"
      },
      "source": [
        "**Cas du Naive Bayes**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqiFYQ_x3NNw"
      },
      "source": [
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator=GaussianNB()\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=estimator, X=X, y=y,\n",
        "                                                       cv=cv, train_sizes=np.linspace(0.1, 1.0, 40),\n",
        "                                                     n_jobs=1)\n",
        "\n",
        "plot_learningcurve(train_scores,test_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-LgPch3fVt"
      },
      "source": [
        "Pour Naïve Bayes, le score de validation et le score d'apprentissage convergent vers une valeur assez faible avec l'augmentation de la taille de l'ensemble d'apprentissage. Cela illustre que le fait d'avoir plus de données d'entrainement ne serait pas utile.  Par contre, cette forme de courbe peut se retrouver souvent dans des jeux de données plus complexes : le score d'entraînement est très élevé au début et diminue et le score de validation croisée est très faible au début et augmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYNcH_CC3mWm"
      },
      "source": [
        "**Cas du SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yXtZyZu3pfu"
      },
      "source": [
        "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
        "estimator = SVC(gamma=0.001)\n",
        "train_sizes, train_scores, test_scores = learning_curve(estimator=estimator, X=X, y=y,\n",
        "                                                       cv=cv, train_sizes=np.linspace(0.1, 1.0, 40),\n",
        "                                                     n_jobs=1)\n",
        "\n",
        "plot_learningcurve(train_scores,test_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqF9XsgV_u8h"
      },
      "source": [
        "À partir de la courbe, nous pouvons voir que plus la taille de l'ensemble d'apprentissage augmente, plus la courbe de score d'apprentissage et la courbe de score de validation croisée convergent. La précision de la validation croisée augmente à mesure que nous ajoutons plus de données d'apprentissage. L'ajout de plus de données d'apprentisage augmente sans doute la généralisation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFZsS4ZGo6C"
      },
      "source": [
        "# Apprentissage profond"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxbnZ7LGAUnk"
      },
      "source": [
        "Pour illustrer la partie apprentissage profond, nous utilisons le jeu de données classiques pima-indians-diabetes qui permet de prédire si des femmes au Perou sont sujet au diabète. Vous pouvez récupérer le fichier en décommantant la ligne suivante :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd4GPfO0xsqx"
      },
      "source": [
        "#!wget http://www.lirmm.fr/~poncelet/Ressources/pima-indians-diabetes.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihMmrLLg0Mnf"
      },
      "source": [
        "# # creation d'un dataframe pour récupérer les données\n",
        "df = pd.read_csv('pima-indians-diabetes.csv', delimiter=',') \n",
        "\n",
        "# Note: 0 — Non Diabetic Patient and 1 — Diabetic Patient\n",
        "\n",
        "# affichage des 5 premières lignes du jeu de données\n",
        "display(df.head())\n",
        "\n",
        "\n",
        "print (\"Repartition des données\",df['Outcome'].value_counts())\n",
        "\n",
        "print (\"Matrice de corrélation\")\n",
        "sns.heatmap(df.corr().round(decimals=2), annot = True, linewidths=.2);\n",
        "\n",
        "class_names=['non diab','diabete']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omz816aWG63U"
      },
      "source": [
        "Dans cette section nous analysons différentes architectures de réseaux de neurones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu0fQP38BRnD"
      },
      "source": [
        "## Cas d'underfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RssVh2LyBVTl"
      },
      "source": [
        "En analysant la matrice de corrélation, nous constatons que *SkinThickness* n'est vraiment pas corrélé à la classe (0.07). L'objectif ici est de montrer qu'avec une telle donnée nous ne sommes pas capable de réaliser un bon modèle de prédiction. Il s'agit juste d'une illustration bien sûr !. Nous résumons donc les variables prédictives à ce seul attribut. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DMx6M6RXqib"
      },
      "source": [
        "X = df[['SkinThickness']]\n",
        "# Il n'est pas nécessaire de normaliser ici car il n'y a qu'un attribut\n",
        "y = df.Outcome\n",
        "\n",
        "# création d'un jeu de test et d'apprentissage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PIFbRz_CMqZ"
      },
      "source": [
        "Nous allons créer tout d'abord un modèle simple composé de 2 neurones dans la première couche. La couche de sortie (avec 1 neurone) contient bien évidemment une *sigmoid* car il s'agit ici d'une classification binaire.  \n",
        "\n",
        "Paramètres (ils ne sont pas forcément optimisés ici) :  \n",
        "* Nombre d'epochs : 20  \n",
        "* Batchsize : 32  \n",
        "* Optimizer : Adam\n",
        "* Loss : binary_crossentropy (classification binaire)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNrN00czCNd_"
      },
      "source": [
        "# definition du modele keras très simple \n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=20, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxmFjt5LDjzI"
      },
      "source": [
        "Les courbes indiquent clairement un **underfitting**. Au niveau du *loss*, nous constatons que la courbe pour la validation est en dessous de celle du training. Elles ne convergent jamais. Nous voyons également que l'*accuracy* ne change pas et que la validation est au dessus. Généralement pour la *loss* lorsque la courbe de validation reste en dessous il s'agit d'un **underfitting**.    \n",
        "\n",
        "Ajouter des epochs à ce modèle ne changera rien comme l'illustre l'expérience suivante avec :   \n",
        "* epochs : 40\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZbCFJGmXs0j"
      },
      "source": [
        "# definition du modele keras très simple \n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiiA0WwrYFeQ"
      },
      "source": [
        "Nous sommes toujours en **underfitting**, il faut soit essayer d'avoir un modèle plus complexe, soit ajouter des attributs (ici nous avons triché ... donc on peut mettre d'autres features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XakHjaAGygo"
      },
      "source": [
        "**Essai avec un modèle plus complexe**  \n",
        "\n",
        "Dans un premier temps nous essayons de rendre le modèle plus complexe. La première couche cachée contient maintenant 8 neurones et nous ajoutons également une couche dense composée de 16 neurones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIpjM7JTEd1O"
      },
      "source": [
        "# definition du modele keras plus complexe \n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTmZe3qVG181"
      },
      "source": [
        "Comme attendu, nous voyons qu'il y a toujours un **underfitting**. En effet les données seules, SkinThickness avec une mauvaise corrélation, ne sont pas suffisante pour faire une bonne prédiction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1gsg9WXHa-j"
      },
      "source": [
        "**Ajout d'autres attributs**  \n",
        "\n",
        "Nous pouvons constater sur la matrice de corrélation que les attributs *Age*, *BMI*,*Glucose* semblent être plus en corrélation avec la variable prédictive (respectivement 0.24, 0.29 et 0.47). Ici il faut faire attention car les valeurs sont sur des domaines très différents. Il faut penser à normaliser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEvi4mxmI6ox"
      },
      "source": [
        "X = df[['Age','BMI','Glucose']]\n",
        "y = df.Outcome\n",
        "# normalisation du jeu de données\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "#y = df[\"Label\"].values\n",
        "# création d'un jeu de test et d'apprentissage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UEE7WD1H8CC"
      },
      "source": [
        "Avec le modèle simple : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcclDi8DH8m8"
      },
      "source": [
        "# definition du modele keras très simple \n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akpUpr-EIECd"
      },
      "source": [
        "Nous constatons qu'il n'y a pas d'amélioration. Le modèle est vraiment trop simple.   \n",
        "\n",
        "Avec le modèle complexe nous avons : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3EQQ_2ZIFIp"
      },
      "source": [
        "# definition du modele keras plus complexe\n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7DHh27YIlkY"
      },
      "source": [
        "Nous pouvons constater que le résultat est nettement meilleur. Par contre vers l'epoch 10, le score loss pour la validation diminue, cela montre qu'il commence à y avoir de l'**overfitting** : le modèle apprend \"par coeur\" les données d'apprentissage. Nous pouvons aussi le constater sur l'accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k44wx0xkI-lZ"
      },
      "source": [
        "## Cas d'overfitting   \n",
        "\n",
        "Lors de la dernière expérimentation nous avons constaté qu'il y avait de l'**overfitting**, i.e. le modèle commence à trop suivre les données et ne généralise pas. Nous allons donc rajouter de la régularisation. Une manière simple consiste à rajouter des *dropout* qui ont pour but de ne pas activer un pourcentage de neurones lors de l'apprentissage.\n",
        "\n",
        "**Ajout de Dropout 0.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC2Knn1uJjgG"
      },
      "source": [
        "# definition du modele keras plus complexe \n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUzu6InTq-HO"
      },
      "source": [
        "Nous voyons que le modèle est nettement meilleur.   \n",
        "\n",
        "**Remarque :** il se peut pour l'accuracy que pour la validation le score soit meilleur que pour l'entrainement. Cela est lié au fait que le modèle, lorsqu'il apprend, applique les dropout alors que dans la validation ces dropout n'apparaissent plus.  \n",
        "\n",
        "Une autre manière de faire de la régularisation est de faire du **EarlyStopping**. Le principe consiste à suivre une mesure (e.g. *val_score*) et si elle n'évolue pas pendant une période donnée, i.e. *patience*, d'arrêter l'apprentissage (C.f. https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping).  \n",
        "\n",
        "\n",
        "**Remarque :** Dans notre contexte particulier, petit jeu de données et avec les variations liées au batchsize, il est difficile de \"tuner\" la patience. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-Wbyqcek1Kk"
      },
      "source": [
        "early_stop = keras.callbacks.EarlyStopping(\n",
        "  monitor='val_accuracy',\n",
        "  patience=7\n",
        ")\n",
        "\n",
        "# definition du modele keras plus complexe \n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32, callbacks=[early_stop])\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuhWXgPmsbQu"
      },
      "source": [
        "Maintenant que nous avons appris un modèle qui se comporte bien nous l'appliquons sur toutes les données pour voir s'il est adapté. Attention, généralement outre le fait de faire de la *feature selection* sur votre jeu de données, vous commencez par l'ensemble des features. Ici nous avons juste décrit quelques exemples d'overfitting et d'underfitting.   \n",
        "\n",
        "**Remarque : ** il faut faire attention car le jeu de données est très déséquilibré (500 vs. 268). Par conséquent le classifieur ne pourra pas être très performant. Dans les expériences précédentes, volontairement, lors du train_test_split, nous n'avons pas sélectionné l'option *stratify=y* cela veut dire qu'avec le déséquilibre la classe moins représenté peut ne pas apparaître suffisamment dans le jeu d'apprentissage. Le fait de faire un échantillonage stratifié oblige à créer un échantillon représentarif en terme de distribution des classes.\n",
        "\n",
        "**Remarque :** lors des expérimentations et dans la suivante, nous effectuons par simplification une seule fois un *train_test_split*. Bien entendu, il faudrait faire les expérimentations avec un K-fold pour que les résultats et donc le modèle soit vraiment représentatif. Dans notre cas il faudrait même utiliser un *StratifiedKfold*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sfajp10l5ki"
      },
      "source": [
        "# # creation d'un dataframe pour récupérer les données\n",
        "df = pd.read_csv('pima-indians-diabetes.csv', delimiter=',') \n",
        "print (df.shape)\n",
        "# Note: 0 — Non Diabetic Patient and 1 — Diabetic Patient\n",
        "\n",
        "class_names=['non diab','diabete']\n",
        "\n",
        "array = df.values\n",
        "X = array[:,0:8] \n",
        "y = array[:,8]\n",
        "\n",
        "# normalisation du jeu de données\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "# création d'un jeu de test et d'apprentissage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y,random_state=0)\n",
        "\n",
        "# definition du modele keras plus complexe \n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilation du modèle\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit le modèle sur le jeu de données. Nous mettons un batchsize de 32\n",
        "history=model.fit(X_train, y_train, validation_data=(X_test,y_test),epochs=40, batch_size=32)\n",
        "\n",
        "y_pred=model.predict(X_test) \n",
        "y_pred[y_pred <= 0.5] = 0.\n",
        "y_pred[y_pred > 0.5] = 1.\n",
        "\n",
        "print (\"\\nRappel du modèle testé\")\n",
        "print (model.summary(),'\\n')\n",
        "conf=confusion_matrix(y_test,y_pred)\n",
        "plot_curves_confusion (history,conf,class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Z2UA8cSFAe"
      },
      "source": [
        "# Utilisation de tensorboard  \n",
        "\n",
        "Tensorflow propose tensorboard pour pouvoir notamment visualiser les courbes et les manipuler.   \n",
        "\n",
        "Il est tout à fait possible de l'utiliser dans un notebook. Il faut au préalable préciser que l'on utilise l'extension tensorboard puis sauvegarder les modèles appris. Enfin via tensorboard on peut analyser les différentes courbes. L'exemple ci-dessous illustre une utilisation de tensorflow sur le jeu de données fashion mnist. L'objectif ici n'est pas trouver un très bon modèle mais plutôt de voir comment utiliser tensorboard.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKfwUcFhx3I5"
      },
      "source": [
        "import tensorflow as tf\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "\n",
        "#effacer les fichiers logs existant\n",
        "!rm -rf ./logs/\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  \n",
        "def train_model():\n",
        "  \n",
        "  model = create_model()\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  model.fit(x=x_train, \n",
        "            y=y_train, \n",
        "            epochs=5, \n",
        "            validation_data=(x_test, y_test), \n",
        "            callbacks=[tensorboard_callback])\n",
        "\n",
        "train_model()\n",
        "\n",
        "%tensorboard --logdir logs\n",
        "\n",
        "from tensorboard import notebook\n",
        "notebook.list()\n",
        "\n",
        "notebook.display(port=6006, height=1000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nAyhoOpUtgs"
      },
      "source": [
        "\n",
        "Nous avions vu précédemment comment évaluer un modèle avec des mesures comme l'accuracy ou la matrice de confusion qui sont toujours à utiliser. Dans ce notebook nous avons vu que tous les modèles recherchent en fait à minimiser une fonction de coût et qu'il faut faire en sorte que notre modèle n'ait pas de bias élevé **Underfit** ni de forte vaiance **Overfit**.   \n",
        "\n",
        "Nous avons vu que les courbes d'apprentissage sont d'une grande aide pour mieux comprendre où se situe notre modèle ainsi que des solutions pour l'améliorer. Dans ce notebook nous n'avons pas traité le cas du *learning_rate*, des *optimizers* qui peuvent, dans les modèles profonds, améliorer l'apprentissage du modèle. Définir un bon modèle est difficile et long. Il ne faut pas hésiter à utiliser les outils qui sont à notre disposition pour mieux comprendre comment le modèle apprend et ainsi l'améliorer ou l'optimiser : un bon modèle doit être simple ... mais pas trop.   \n",
        "\n",
        "Le lecteur intéressé pourra se reporter à l'URL suivante : https://pythonprogramming.net/tensorboard-optimizing-models-deep-learning-python-tensorflow-keras/?completed=/tensorboard-analysis-deep-learning-python-tensorflow-keras/ qui illustre un très bon exemple de l'utilisation de tensorboard (et notamment des learning curves que nous avons vu dans ce notebook) pour optimiser un modèle. "
      ]
    }
  ]
}